{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "CHROMA_HOST = os.environ.get(\"CHROMA_HOST\")\n",
    "\n",
    "openai_client = OpenAI(\n",
    "    api_key=OPENAI_API_KEY\n",
    ")\n",
    "\n",
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    model_name=\"text-embedding-ada-002\"\n",
    ")\n",
    "chroma_client = chromadb.HttpClient(\n",
    "    host=CHROMA_HOST, \n",
    "    port=8000\n",
    ")\n",
    "\n",
    "collection = chroma_client.get_collection(\n",
    "    name='content_collection', \n",
    "    embedding_function=openai_ef\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(query_texts):\n",
    "\n",
    "    try:\n",
    "        # Ensure that query_texts is a list\n",
    "        if not isinstance(query_texts, list):\n",
    "            query_texts = [query_texts]\n",
    "\n",
    "\n",
    "        results = collection.query(query_texts=query_texts, n_results=2)\n",
    "\n",
    "        documents = []\n",
    "        for i, document_list in enumerate(results['documents']):\n",
    "            for j, document in enumerate(document_list):\n",
    "                if results['distances'][i][j] < 0.5:\n",
    "                    metadata = results['metadatas'][i][j]\n",
    "                    documents.append({'document': document, 'metadatas': metadata})\n",
    "\n",
    "        if not documents:\n",
    "            print(\"No relevant documents\")\n",
    "            return \"No relevant documents\"\n",
    "\n",
    "        return documents\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during query: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_model(input_df):\n",
    "    responses = []\n",
    "    for index, row in input_df.iterrows():\n",
    "        questions = row[\"questions\"]\n",
    "        context = query(questions) \n",
    "        system_prompt = \"\"\"\n",
    "        You are a helpful assistant who can answer question with information from the provided context, which comes from our platform content. \n",
    "        Make sure to only use information from the provided documents. \n",
    "        If the context does not have the information to answer the question reply `I'm sorry but I'm not sure about that.`\n",
    "        Do not respond with anything outside of the scope of financial wellness or financial literacy. \n",
    "        \"\"\"\n",
    "\n",
    "        completion = openai_client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo-1106\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": f\"{system_prompt}\"},\n",
    "                {\"role\": \"user\", \"content\": f\"The user question:{questions} \\n The context:{context}\"}\n",
    "            ]\n",
    "        )\n",
    "        response = completion.choices[0].message.content\n",
    "        responses.append({\"system_prompt\": system_prompt, \"response\": response, \"context\": context})\n",
    "\n",
    "    return responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.DataFrame(\n",
    "    {\n",
    "        \"questions\": [\n",
    "            \"What exactly is credit score?\",\n",
    "            \"Give me a couple tips on how to budget.\",\n",
    "            \"Who won the Super Bowl?\",\n",
    "            \"What are the pros and cons of getting a credit card?\"\n",
    "        ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ChristianReetz\\OneDrive - Aztec Software\\Desktop\\ai\\env\\Lib\\site-packages\\mlflow\\data\\digest_utils.py:26: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  string_columns = trimmed_df.columns[(df.applymap(type) == str).all(0)]\n",
      "c:\\Users\\ChristianReetz\\OneDrive - Aztec Software\\Desktop\\ai\\env\\Lib\\site-packages\\mlflow\\models\\evaluation\\base.py:414: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  data = data.applymap(_hash_array_like_element_as_bytes)\n",
      "2023/12/15 12:59:15 INFO mlflow.models.evaluation.base: Evaluating the model with the default evaluator.\n",
      "2023/12/15 12:59:15 INFO mlflow.models.evaluation.default_evaluator: Computing model predictions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User question: ['What exactly is credit score?']\n",
      "Distance of documents: 0.18243224918842316\n",
      "-------\n",
      "Distance of documents: 0.2285841703414917\n",
      "-------\n",
      "User question: ['Give me a couple tips on how to budget.']\n",
      "Distance of documents: 0.22857920825481415\n",
      "-------\n",
      "Distance of documents: 0.23825645446777344\n",
      "-------\n",
      "User question: ['Who won the Super Bowl?']\n",
      "Distance of documents: 0.48363224713635566\n",
      "-------\n",
      "Distance of documents: 0.48660082434717156\n",
      "-------\n",
      "User question: ['What are the pros and cons of getting a credit card?']\n",
      "Distance of documents: 0.1996995508670807\n",
      "-------\n",
      "Distance of documents: 0.20346784591674805\n",
      "-------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/12/15 12:59:26 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n",
      "2023/12/15 12:59:26 WARNING mlflow.metrics.metric_definitions: Failed to load 'toxicity' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n",
      "2023/12/15 12:59:26 WARNING mlflow.metrics.metric_definitions: Failed to load flesch kincaid metric, skipping metric logging.\n",
      "2023/12/15 12:59:26 WARNING mlflow.metrics.metric_definitions: Failed to load automated readability index metric, skipping metric logging.\n",
      "2023/12/15 12:59:26 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: token_count\n",
      "2023/12/15 12:59:26 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: toxicity\n",
      "2023/12/15 12:59:26 WARNING mlflow.metrics.metric_definitions: Failed to load 'toxicity' metric (error: ModuleNotFoundError(\"No module named 'evaluate'\")), skipping metric logging.\n",
      "2023/12/15 12:59:26 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: flesch_kincaid_grade_level\n",
      "2023/12/15 12:59:26 WARNING mlflow.metrics.metric_definitions: Failed to load flesch kincaid metric, skipping metric logging.\n",
      "2023/12/15 12:59:26 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: ari_grade_level\n",
      "2023/12/15 12:59:26 WARNING mlflow.metrics.metric_definitions: Failed to load automated readability index metric, skipping metric logging.\n",
      "2023/12/15 12:59:26 INFO mlflow.models.evaluation.default_evaluator: Evaluating builtin metrics: exact_match\n"
     ]
    }
   ],
   "source": [
    "results = mlflow.evaluate(\n",
    "    gpt_model,\n",
    "    eval_df,  \n",
    "    model_type=\"question-answering\",\n",
    "    predictions=\"response\",\n",
    "    evaluator_config={\n",
    "        \"col_mapping\": {\n",
    "            \"system_prompt\": \"system_prompt\",\n",
    "            \"inputs\": \"questions\",  \n",
    "            \"context\": \"source_documents\",  \n",
    "        }\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6f53767b2c341f7bc33d7dfd7cb16e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>questions</th>\n",
       "      <th>outputs</th>\n",
       "      <th>system_prompt</th>\n",
       "      <th>context</th>\n",
       "      <th>token_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What exactly is credit score?</td>\n",
       "      <td>A credit score is essentially a measure of the...</td>\n",
       "      <td>\\n        You are a helpful assistant who can ...</td>\n",
       "      <td>[{'document': 'Credit Scores ', 'metadatas': {...</td>\n",
       "      <td>126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Give me a couple tips on how to budget.</td>\n",
       "      <td>Based on the provided context, here are a coup...</td>\n",
       "      <td>\\n        You are a helpful assistant who can ...</td>\n",
       "      <td>[{'document': 'Budgeting can feel like a tedio...</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Who won the Super Bowl?</td>\n",
       "      <td>I'm sorry but I'm not sure about that.</td>\n",
       "      <td>\\n        You are a helpful assistant who can ...</td>\n",
       "      <td>[{'document': 'Vacations ', 'metadatas': {'Res...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are the pros and cons of getting a credit...</td>\n",
       "      <td>The pros of getting a credit card include conv...</td>\n",
       "      <td>\\n        You are a helpful assistant who can ...</td>\n",
       "      <td>[{'document': 'Do I Need a Credit Card? Host: ...</td>\n",
       "      <td>216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           questions  \\\n",
       "0                      What exactly is credit score?   \n",
       "1            Give me a couple tips on how to budget.   \n",
       "2                            Who won the Super Bowl?   \n",
       "3  What are the pros and cons of getting a credit...   \n",
       "\n",
       "                                             outputs  \\\n",
       "0  A credit score is essentially a measure of the...   \n",
       "1  Based on the provided context, here are a coup...   \n",
       "2             I'm sorry but I'm not sure about that.   \n",
       "3  The pros of getting a credit card include conv...   \n",
       "\n",
       "                                       system_prompt  \\\n",
       "0  \\n        You are a helpful assistant who can ...   \n",
       "1  \\n        You are a helpful assistant who can ...   \n",
       "2  \\n        You are a helpful assistant who can ...   \n",
       "3  \\n        You are a helpful assistant who can ...   \n",
       "\n",
       "                                             context  token_count  \n",
       "0  [{'document': 'Credit Scores ', 'metadatas': {...          126  \n",
       "1  [{'document': 'Budgeting can feel like a tedio...          105  \n",
       "2  [{'document': 'Vacations ', 'metadatas': {'Res...           11  \n",
       "3  [{'document': 'Do I Need a Credit Card? Host: ...          216  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.tables[\"eval_results_table\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
